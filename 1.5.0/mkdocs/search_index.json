{
    "docs": [
        {
            "location": "/", 
            "text": "Optimized Analytics Package (OAP) is an open source project to optimize Apache Spark on SQL engine, MLlib and so on, driven by Intel and the community.\n\n\nWhy use OAP?\n\n\nApache Spark is powerful and well optimized on many aspects, but it still faces some challenges to achieve a higher-level performance.\n\n\n\n\n\n\nThe JVM and row-based computing engine prevents Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU.\n\n\n\n\n\n\nThe current implementation of key aspects, such as memory management \n shuffle, doesn't consider the latest technology advancements,  like PMEM.\n\n\n\n\n\n\nThe batch processing engine cannot satisfy the need of queries with high performance requirement.\n\n\n\n\n\n\nOAP Project aims to optimize Spark on these aspects above. It had 6 components, including \nGazelle Plugin\n, \nOAP MLlib\n, \nSQL DS Cache\n, \nPMem Spill\n, \nPMem Common\n, and \nPMem Shuffle\n in previous releases.\n\n\nSince 1.4.0, OAP consists of 3 components: \nGazelle Plugin\n, \nOAP MLlib\n and \nCloudTik\n. \n\n\n\n\nHow to use OAP?\n\n\nGuide\n\n\nPlease refer to OAP project installation and developer guide below for instructions.\n\n\n\n\nOAP Installation Guide\n\n\nOAP Developer Guide\n\n\n\n\nComponents\n\n\nYou can get more detailed information from each component web page of OAP Project below.\n\n\n\n\nGazelle Plugin\n\n\nOAP MLlib\n\n\nCloudTik", 
            "title": "Overview"
        }, 
        {
            "location": "/#why-use-oap", 
            "text": "Apache Spark is powerful and well optimized on many aspects, but it still faces some challenges to achieve a higher-level performance.    The JVM and row-based computing engine prevents Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU.    The current implementation of key aspects, such as memory management   shuffle, doesn't consider the latest technology advancements,  like PMEM.    The batch processing engine cannot satisfy the need of queries with high performance requirement.    OAP Project aims to optimize Spark on these aspects above. It had 6 components, including  Gazelle Plugin ,  OAP MLlib ,  SQL DS Cache ,  PMem Spill ,  PMem Common , and  PMem Shuffle  in previous releases.  Since 1.4.0, OAP consists of 3 components:  Gazelle Plugin ,  OAP MLlib  and  CloudTik .", 
            "title": "Why use OAP?"
        }, 
        {
            "location": "/#how-to-use-oap", 
            "text": "", 
            "title": "How to use OAP?"
        }, 
        {
            "location": "/#guide", 
            "text": "Please refer to OAP project installation and developer guide below for instructions.   OAP Installation Guide  OAP Developer Guide", 
            "title": "Guide"
        }, 
        {
            "location": "/#components", 
            "text": "You can get more detailed information from each component web page of OAP Project below.   Gazelle Plugin  OAP MLlib  CloudTik", 
            "title": "Components"
        }, 
        {
            "location": "/Components/", 
            "text": "OAP Project aims to optimize Spark, and it contains 3 components including \nGazelle Plugin\n, \nOAP MLlib\n and \nCloudTik\n since release 1.4.0.\n\n\n\n\n\n\nGazelle Plugin\n\n\n\n\n\n\nOAP MLlib\n\n\n\n\n\n\nCloudTik\n\n\n\n\n\n\nGazelle Plugin\n\n\nDocuments Website\n\n\nSpark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However, Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries.\nApache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient.\n\n\nGazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow,\nand leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.\n\n\nOAP MLlib\n\n\nDocuments Website\n\n\nOAP MLlib is an optimized package to accelerate machine learning algorithms in Spark MLlib.\nIt is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities.\nIt also takes advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.\n\n\nCloudTik\n\n\nDocuments Website\n\n\nCloudTik is a cloud scaling platform to scale your distributed analytics and AI cluster on public cloud providers including AWS, Azure, GCP, and so on.\n\n\n\n\n\n\nBuilt upon Cloud compute engines and Cloud storages\n\n\n\n\n\n\nSupport major public Cloud providers (AWS, Azure, GCP, and more to come)\n\n\n\n\n\n\nPowerful and Optimized: Out of box and optimized runtimes for Analytics and AI\n\n\n\n\n\n\nSimplified and Unified: Easy to use and unified operating experience on all Cloud providers\n\n\n\n\n\n\nOpen and Flexible: Open architecture and users with full control, fully open-source and user transparent.", 
            "title": "Components"
        }, 
        {
            "location": "/Components/#gazelle-plugin", 
            "text": "Documents Website  Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However, Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries.\nApache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD-optimized kernels and LLVM-based SQL engine Gandiva are also very efficient.  Gazelle Plugin reimplements Spark SQL execution layer with SIMD-friendly columnar data processing based on Apache Arrow,\nand leverages Arrow's CPU-cache friendly columnar in-memory layout, SIMD-optimized kernels and LLVM-based expression engine to bring better performance to Spark SQL.", 
            "title": "Gazelle Plugin"
        }, 
        {
            "location": "/Components/#oap-mllib", 
            "text": "Documents Website  OAP MLlib is an optimized package to accelerate machine learning algorithms in Spark MLlib.\nIt is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities.\nIt also takes advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.", 
            "title": "OAP MLlib"
        }, 
        {
            "location": "/Components/#cloudtik", 
            "text": "Documents Website  CloudTik is a cloud scaling platform to scale your distributed analytics and AI cluster on public cloud providers including AWS, Azure, GCP, and so on.    Built upon Cloud compute engines and Cloud storages    Support major public Cloud providers (AWS, Azure, GCP, and more to come)    Powerful and Optimized: Out of box and optimized runtimes for Analytics and AI    Simplified and Unified: Easy to use and unified operating experience on all Cloud providers    Open and Flexible: Open architecture and users with full control, fully open-source and user transparent.", 
            "title": "CloudTik"
        }, 
        {
            "location": "/OAP-Installation-Guide/", 
            "text": "This document introduces how to install OAP and its dependencies on your cluster nodes by \nConda\n. \nFollow steps below on \nevery node\n of your cluster to set right environment for each machine.\n\n\nContents\n\n\n\n\nPrerequisites\n\n\nInstalling OAP\n\n\nConfiguration\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nOS Requirements\n\nWe have tested OAP on Fedora 29, CentOS 7.6 (kernel 4.18.16) and Ubuntu 20.04 (kernel 5.4.0-65-generic). \n\n\n\n\n\n\nConda Requirements\n \n\nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:\n\n\n\n\n\n\n$ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh \n\n\n\n\nFor changes to take effect, \nclose and re-open\n your current shell. \nTo test your installation,  run the command \nconda list\n in your terminal window. A list of installed packages appears if it has been installed correctly.\n\n\nInstalling OAP\n\n\nCreate a Conda environment and install OAP Conda package.\n\n\n$ conda create -n oapenv -c conda-forge -c intel -y oap=1.5.0.spark32\n\n\n\n\nOnce finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under \n$HOME/miniconda2/envs/oapenv/oap_jars\n\n\nConfiguration\n\n\nOnce finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to \n$SPARK_HOME/conf/spark-defaults.conf\n.\n\n\nspark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executorEnv.CC                $HOME/miniconda2/envs/oapenv/bin/gcc\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\n\n\n\n\nThen you can follow the corresponding feature documents for more details to use them.", 
            "title": "Installation Guide"
        }, 
        {
            "location": "/OAP-Installation-Guide/#contents", 
            "text": "Prerequisites  Installing OAP  Configuration", 
            "title": "Contents"
        }, 
        {
            "location": "/OAP-Installation-Guide/#prerequisites", 
            "text": "OS Requirements \nWe have tested OAP on Fedora 29, CentOS 7.6 (kernel 4.18.16) and Ubuntu 20.04 (kernel 5.4.0-65-generic).     Conda Requirements   \nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:    $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh   For changes to take effect,  close and re-open  your current shell. \nTo test your installation,  run the command  conda list  in your terminal window. A list of installed packages appears if it has been installed correctly.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/OAP-Installation-Guide/#installing-oap", 
            "text": "Create a Conda environment and install OAP Conda package.  $ conda create -n oapenv -c conda-forge -c intel -y oap=1.5.0.spark32  Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under  $HOME/miniconda2/envs/oapenv/oap_jars", 
            "title": "Installing OAP"
        }, 
        {
            "location": "/OAP-Installation-Guide/#configuration", 
            "text": "Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to  $SPARK_HOME/conf/spark-defaults.conf .  spark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executorEnv.CC                $HOME/miniconda2/envs/oapenv/bin/gcc\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar  Then you can follow the corresponding feature documents for more details to use them.", 
            "title": "Configuration"
        }, 
        {
            "location": "/OAP-Developer-Guide/", 
            "text": "This document contains the instructions \n scripts on installing necessary dependencies and building OAP modules. \nYou can get more detailed information from OAP each module below.\n\n\n\n\nOAP MLlib\n\n\nGazelle Plugin\n\n\n\n\nBuilding OAP\n\n\nPrerequisites\n\n\nWe provide scripts to help automatically install dependencies required, please change to \nroot\n user and run:\n\n\n# git clone -b \ntag-version\n https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh\n\n\n\n\nNote\n: oap-tools tag version \nv1.5.0\n corresponds to  all OAP modules' tag version \nv1.5.0\n.\n\n\nThen the dependencies below will be installed:\n\n\n\n\nCmake\n\n\nGCC \n 7\n\n\nOneAPI\n\n\nArrow\n\n\nLLVM\n \n\n\n\n\nBuilding\n\n\nBuilding OAP\n\n\nOAP is built with \nApache Maven\n and Oracle Java 8.\n\n\nTo build OAP package, run command below then you can find a tarball named \noap-$VERSION-*.tar.gz\n under directory \n$OAP_TOOLS_HOME/dev/release-package\n, which contains all OAP module jars.\nChange to \nroot\n user, run\n\n\n# cd oap-tools\n# sh dev/compile-oap.sh\n\n\n\n\nBuilding OAP specific module\n\n\nIf you just want to build a specific OAP Module, such as \ngazelle_plugin\n, change to \nroot\n user, then run:\n\n\n# cd oap-tools\n# sh dev/compile-oap.sh --component=gazelle_plugin", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building-oap", 
            "text": "", 
            "title": "Building OAP"
        }, 
        {
            "location": "/OAP-Developer-Guide/#prerequisites", 
            "text": "We provide scripts to help automatically install dependencies required, please change to  root  user and run:  # git clone -b  tag-version  https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh  Note : oap-tools tag version  v1.5.0  corresponds to  all OAP modules' tag version  v1.5.0 .  Then the dependencies below will be installed:   Cmake  GCC   7  OneAPI  Arrow  LLVM", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building", 
            "text": "", 
            "title": "Building"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building-oap_1", 
            "text": "OAP is built with  Apache Maven  and Oracle Java 8.  To build OAP package, run command below then you can find a tarball named  oap-$VERSION-*.tar.gz  under directory  $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars.\nChange to  root  user, run  # cd oap-tools\n# sh dev/compile-oap.sh", 
            "title": "Building OAP"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building-oap-specific-module", 
            "text": "If you just want to build a specific OAP Module, such as  gazelle_plugin , change to  root  user, then run:  # cd oap-tools\n# sh dev/compile-oap.sh --component=gazelle_plugin", 
            "title": "Building OAP specific module"
        }, 
        {
            "location": "/OAP-Release-Cadence/", 
            "text": "Starting with OAP 1.1.0, OAP project follows the semantic versioning guidelines with a few deviations. These small differences account for OAP\u2019s nature as a multi-module project.\n\n\nOAP Versions\n\n\nEach OAP release will be versioned: [MAJOR].[FEATURE].[MAINTENANCE]\n\n\nRelease Cadence\n\n\nIn general, major feature releases occur about every 4 months. Hence, OAP would generally be released about 4 months after 1.1.0. Maintenance releases happen as needed in between feature releases. Major releases do not happen according to a fixed schedule.\n\n\nOAP 1.6 Release Window\n\n\n\n\n\n\n\n\nDate\n\n\nEvent\n\n\n\n\n\n\n\n\n\n\nMay        2023\n\n\nCode freeze. Release branch cut.\n\n\n\n\n\n\nMid June   2023\n\n\nQA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged.\n\n\n\n\n\n\nJune     2023\n\n\nRelease candidates (RC), etc. until final release passes\n\n\n\n\n\n\n\n\nMaintenance Releases\n\n\nFeature release branches will, generally, be maintained with bug fix releases for a period of several months.", 
            "title": "Release Cadence"
        }, 
        {
            "location": "/OAP-Release-Cadence/#oap-versions", 
            "text": "Each OAP release will be versioned: [MAJOR].[FEATURE].[MAINTENANCE]", 
            "title": "OAP Versions"
        }, 
        {
            "location": "/OAP-Release-Cadence/#release-cadence", 
            "text": "In general, major feature releases occur about every 4 months. Hence, OAP would generally be released about 4 months after 1.1.0. Maintenance releases happen as needed in between feature releases. Major releases do not happen according to a fixed schedule.", 
            "title": "Release Cadence"
        }, 
        {
            "location": "/OAP-Release-Cadence/#oap-16-release-window", 
            "text": "Date  Event      May        2023  Code freeze. Release branch cut.    Mid June   2023  QA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged.    June     2023  Release candidates (RC), etc. until final release passes", 
            "title": "OAP 1.6 Release Window"
        }, 
        {
            "location": "/OAP-Release-Cadence/#maintenance-releases", 
            "text": "Feature release branches will, generally, be maintained with bug fix releases for a period of several months.", 
            "title": "Maintenance Releases"
        }, 
        {
            "location": "/Presentations/", 
            "text": "Talks:\n\n\n\n\nSpark Native SQL Engine, \nSlidesTalk\n, May 2021, (\nslides\n)\n\n\nBuilding a SIMD Supported Vectorized Native Engine for Spark SQL, \nSpark Data + AI Summit\n, November 2020, (\nslides\n)\n\n\nOAP - SQL Index \n Data Source Cache, \nSlidesTalk\n, August 2020, (\nslides\n)\n\n\nAccelerate Your Spark Using Intel Optane DC Persistent Memory, \nSlidesTalk\n, November 2018, (\nslides\n)\n\n\nOAP: Optimized Analytics Package for Spark Platform, \nSpark Summit\n, October 2017, (\nslides\n)\n\n\n\n\nIntel Website:\n \n\n\n\n\nOptimized Analytics Package for Spark\n Platform (OAP for Spark\n Platform)", 
            "title": "Presentations"
        }, 
        {
            "location": "/Powered-by/", 
            "text": "Alibaba Cloud\n\n\nTIANCHI Competition - E-MapReduce Geek Challenge\n\n\nSpark Performance Optimization Solution based on Intel\u00ae Optimized Analytics Package\n\n\n\n\n\n\nBaidu\n\n\nBaidu BigSQL Delivers Faster Spark Interactive Queries with OAP \n Intel\u00ae Optane\u2122 Persistent Memory\n\n\n\n\n\n\nBONC\n\n\nA Big Data Platform Solution Based on BONC Enterprise Edition Hadoop and Intel\u00ae Optane\u2122 Persistent Memory\n\n\n\n\n\n\nIntel\n \n\n\nSpark Tuning Guide on 3rd Generation Intel\u00ae Xeon\u00ae Scalable Processors Based Platform\n \n\n\n\n\n\n\nLenovo\n\n\nLenovo LeapHD High-Performance Big Data All-in-One Solution with Intel Optimized Analytics Package", 
            "title": "Powered By"
        }
    ]
}
